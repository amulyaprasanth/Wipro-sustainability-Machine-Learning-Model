# -*- coding: utf-8 -*-
"""wipro-competition-tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bC6F2rs4sP-w2aO8niZpp-tHdyDWUgxc
"""

# Standard imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow import feature_column
from sklearn.preprocessing import StandardScaler

"""## Steps we will follow in this notebook

* Download the data
* Become one with the data
* Preprocess the data
* Create a model
* Compile the model
* Fit the model 
* Evaluate the model
"""

# Import the train data
train = pd.read_csv("https://raw.githubusercontent.com/amulyaprasanth/Wipro-sustainability-Machine-Learning-Model/main/train.csv?token=GHSAT0AAAAAABQFXU7ALXBH52PHIX2WJAGMYP2OARQ")
train.head()

# Import the test data
test = pd.read_csv("https://raw.githubusercontent.com/amulyaprasanth/Wipro-sustainability-Machine-Learning-Model/main/test.csv?token=GHSAT0AAAAAABQFXU7AEO63CT7XAEELZYL6YP2OBPA")
test.head()

# These are the target variables which we have to predict
target_labels = ["Clearsky DHI", "Clearsky DNI", "Clearsky GHI"]

"""## Become one with the data"""

train.info()

test.info()

"""#### Splitting the training data into train and validation sets

"""

# First we split the training data into training and validation sets
X_train, y_train, X_valid, y_valid = train.drop(target_labels, axis=1)[:156000], train[target_labels][:156000], train.drop(target_labels, axis=1)[156000:], train[target_labels][156000:]

X_train.head()

y_train

X_train.shape[-1:]

# preprocessing the test data
test_data = test.drop(target_labels, axis=1)

# Normalising our data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
test_data = scaler.transform(test_data)

X_train.shape[-1]

"""## This is our baseline model 

Model is  stored as tensorflow_model0 in google Drive
"""

# Set random seed
tf.random.set_seed(42)

# Create the model
model = Sequential()
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(3))

# Compile the model
model.compile(loss="mean_squared_error",
            optimizer='adam',
            metrics=['mean_squared_error'])

# Fit data to model
model.fit(X_train, y_train,
        batch_size=32,
        epochs=100,
        validation_data=[X_valid, y_valid],
        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])

# Generate generalization metrics
score = model.evaluate(X_valid, y_valid, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')

"""## Experimenatation

Now we perform all types of experiments in this section to beat our baseline

### Model_1: Let's create a neural network with CNN 1D layers

#### Creating a submissions file
"""

# Predicting on test data
preds = model.predict(test_data)

pred1 = preds[:, 0]
pred2 = preds[:, 1]
pred3 = preds[:, 2]

submissions = pd.DataFrame()

submissions['Clearsky DHI'] = pred1
submissions['Clearsky DNI'] = pred2
submissions['Clearsky GHI'] = pred3

submissions.to_csv("tensorflow_model_results.csv", index=False)

